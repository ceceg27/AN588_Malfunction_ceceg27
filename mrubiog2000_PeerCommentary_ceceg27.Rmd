---
title: "mrubiog2000_PeerCommentary_ceceg27"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#homework 4
[1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:
```{r}
z.prop.test <- function(p1, n1, p0, p2= NULL, n2= NULL, alternative="two.tailed", conf.level=0.95){
  #error messages for rule of thumb
   if (n1*p0 < 5)
  {
  print("Non Normal!")
  }
if (n1*(1-p0) < 5)
  {
  print("Non Normal!")
}
  
##Miguel: I think here you need to use "p1" instead of "p0", because you want to test if you can assume normality on "p1" (your sample proportion) rather than on "p0" (population proportion). Apart from that, the structure is good!
  
#these only need to show up if using p2 and n2
if (!is.null(n2))
{
  if (n2*p0 < 5)
  {
  print("Non Normal!")
  }
  if (n2*(1-p0) < 5)
  {
  print("Non Normal!")
  }
  
  ##Miguel: Same as above.
  
  
  #function for z test if there are no errors
  if (alternative == "two.tailed")
  {
    z<- (p1-p0)/sqrt((p0*(1-p0))/n1) #z score
    p<- pnorm(z, lower.tail = TRUE) #p value
    lwr <- (p1-qnorm(conf.level) * sqrt(p1*(1-p1))/n1) #confidence intervals
    upr <- (p1+qnorm(conf.level) * sqrt(p1*(1-p1))/n1)
    ci <- c(lwr, upr)
  }
  
  ##Miguel: Here you would need to change the argument of the "qnorm()" function. Here you are calculating in both cases the 0.95 quantile (your defaut) in the normal distribution. However, we need to apply qnorm on the quantiles 0.975 and 0.025 (in all 0.05 that would be alpha) to get a confidence level of 0.95%. You can try the following:  "qnorm(1 - ((1-conf.level)/2))".
  
#this is for one sample, when n2 and p2 are null
  if (is.null(n2)|is.null(p2))
  {
    z<- (p1-p0)/sqrt((p0*(1-p0))/n1)
    p<- pnorm(z, lower.tail = TRUE)
    lwr <- (p1-qnorm(conf.level) * sqrt(p1*(1-p1))/n1)
    upr <- (p1+qnorm(conf.level) * sqrt(p1*(1-p1))/n1)
    ci <- c(lwr, upr)
  }
#this is for two sample, if p1 is less than p2
    if (alternative == "less")
  {
    pstar<- (sum(p1*n1)+sum(p2*n2))/(n1+n2) #for a pooled proportion
    z<- (p2 - p1)/sqrt((pstar * (1-pstar)) * (1/length(n1) + 1/length(n2))) #z score
    
    ##Miguel: I would say n1 and n2 are already the length of your data, so you will define them when writing the code in which you want to run this function: "z.prop.test(p1 = __, n1 = lentgh(data)...)" so doing it here again may cause an error. The rest seems pretty good!
    
    p<- 1-pnorm(z, lower.tail = T) + pnorm(z,lower.tail = F) #p value
    lwr <- ((p2-p1)-qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1) # ci intervals
    upr <- ((p2-p1)+qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1)
    ci <- c(lwr, upr)
  }
 #this is for two sample, if p1 is greater than p2
   if (alternative == "greater")
  {
    pstar<- (sum(p1*n1)+sum(p2*n2))/(n1+n2) #for a pooled proportion
    z<- (p1 - p2)/sqrt((pstar * (1-pstar)) * (1/length(n1) + 1/length(n2))) #z score
    p<- 1-pnorm(z, lower.tail = T) + pnorm(z,lower.tail = F) #p value
    lwr <- ((p1-p2)-qnorm(conf.level) * sqrt((p1-p2)*(1-(p1-p2)))/n1) #ci intervals
    upr <- ((p1-p2)+qnorm(conf.level) * sqrt((p1-p2)*(1-(p1-p2)))/n1)
    ci <- c(lwr, upr)
  }
values<- c("z-stat", z, "p value", p, "conf. intervals", ci)
print(values)
}
}

```

##Miguel: The structure of the function was not bad but I personally think it was a bit complicated to understand. Maybe you can save some lines of code by applying one single formula to  every test. I am not sure if the tests are set correctly because I had trouble writing them too, so probably Victoria may help you on that one (she did with my code). Anyways, I think you did a really good job! This question was not easy.



[2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):

```{r}
#bring in packages
library(curl)
library(ggplot2)

#load data
f <- curl('https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv')
d <- read.csv(f, stringsAsFactors = FALSE, header = TRUE)
head(d)

#prep the data
#remove na
c<-na.omit(d) 
#make it a data frame so ggplot likes it
h<-data.frame(c) 
#name our variables so we dont have to keep retyping them 
x<-d$MaxLongevity_m 
y<-d$Brain_Size_Species_Mean 
```

```{r}
#making the regular model, using lm
M1<-lm(data = d, y~x) 
M1
summary(M1)
#r2 is .4887

#making the ggplot of the first model 
ggM1<-ggplot(data = d, aes(x = x, y = y)) + geom_point() + geom_smooth(method = "lm", formula = y ~ x)
ggM1
```

##Miguel: Nice plot. You missed the formula expression for your model. You can write it on your plot using the function "annotate("text", ...). 


Identify and interpret the point estimate of the slope (β1
), as well as the outcome of the test associated with the hypotheses H0: β1
 = 0; HA: β1
 ≠ 0. Also, find a 90 percent CI for the slope (β1
) parameter.
```{r}
#finding beta0 and beta1 
t1 <- unlist(M1$coefficients)
# unlist to get coefs out of the model
beta0<-round(t1[1],digits = 2)
beta0
beta1<-round(t1[2],digits = 2)
beta1
#find the 90% confidence intervals
ci.slope<-confint(M1, level = 0.9)
ci.slope
```

##Miguel: You calculated beta1 correctly! but you did not try to explain the hypotheses.

Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.
```{r}
#confidence intervals for 90%
ci <- predict(M1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  
ci.frame<-data.frame(ci)

#prediction intervals for 90%
pi <- predict(M1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  
pi.frame<-data.frame(pi)

##Miguel: Good job! With your code I realised I only calculate the prediction and not the actual CI.



#also need to combine the original x and y values
r<-cbind(x, y) 
#now we can combine the x and y values, and the ci and pi data frames
New<-cbind(r, ci.frame, pi.frame)
#renaming the columns
names(New) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(New)
#making the ggplot
gMod2<-ggplot(data = New, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = New, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = New, aes(x = x, y = CIlower), colour = "blue") +
  geom_line(data = New, aes(x = x, y = CIupper), colour = "blue") +
  geom_line(data = New, aes(x = x, y = PIlower), colour = "red") +
  geom_line(data = New, aes(x = x, y = PIupper), colour = "red")
gMod2
#upper and lower ci and pi are on the graph!
```

##Miguel: Nice! When seeing your plot I realised that a did not know by first sight which line was the predicted and the CI and that it happens the same to me (we did not set the color legend, ups).

Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
```{r}
#point estimate
predict(M1, newdata = data.frame(x = 800))
#result is 258.2979
#PIs
predict(M1, newdata = data.frame(x = 800), interval = "prediction",level = 0.90)
#fit = 258.2979, lwr = 166.6757, upr = 349.9201
#I would not expect this to be accurate, as the model is meant to predict brain sizes and longevity of animals with similar brain sizes. Because 800 grams is so much more than the brain sizes in the data set, I don't think its accurate to assume the same relationship would exist with an extremely large value. 
```
#now lets do it all for the log model 
```{r}
#log transform the variables and make them into a data frame 
r1<-log(d$MaxLongevity_m)
r2<-log(d$Brain_Size_Species_Mean)
df2<-as.data.frame(cbind(r2,r1))

#making the transformed model
logM1<-lm(data = d, log(y)~log(x))
logM1
summary(logM1)
#the r2 is now 0.5751, higher than it was for the regular model

#making the ggplot
ggM2 <- ggplot(data=info,aes(x=r2,y=r1))+xlab("log(Brain_Size_Species_Mean)")+ylab("log(MaxLongevity_m)")+ geom_point() + geom_smooth(method="lm", fullrange=TRUE)
ggM2

```

##Miguel: I am not sure why do we need to do the "log" replicate, but the structure looks good. The only thing is that when I tried to run the chunck I got 2 errors "Error in ggplot(data = info, aes(x = r2, y = r1)):   object 'info' not found" AND "Error: object 'ggM2' not found". I tried to look for an "info" object around your code but I did not find it :/ Probably the second error appears because R cannot set ggM2 due to the first error.
##I tried typeing "data = d" instead and I got the plot.

```{r}
#finding beta0 and beta1 
t1 <- unlist(logM1$coefficients)
# unlist to get coefs out of the model
beta0<-round(t1[1],digits = 2)
beta0
beta1<-round(t1[2],digits = 2)
beta1
#find the 90% confidence intervals
ci.slope<-confint(M1, level = 0.9)
ci.slope
```

```{r}
#confidence intervals for 90%
ci <- predict(logM1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  
logci.frame<-data.frame(ci)

#prediction intervals for 90%
pi <- predict(logM1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  
logpi.frame<-data.frame(pi)

#also need to combine the original x and y values
r<-cbind(x, y) 
#now we can combine the x and y values, and the ci and pi data frames
logNew<-cbind(r, ci.frame, pi.frame)
#renaming the columns
names(logNew) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(logNew)
#making the ggplot
loggMod2<-ggplot(data = logNew, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = logNew, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = logNew, aes(x = x, y = CIlower), colour = "blue") +
  geom_line(data = logNew, aes(x = x, y = CIupper), colour = "blue") +
  geom_line(data = logNew, aes(x = x, y = PIlower), colour = "red") +
  geom_line(data = logNew, aes(x = x, y = PIupper), colour = "red")
loggMod2
```

```{r}
#point estimate
predict(logM1, newdata = data.frame(x = 800))
#result is 258.2979
#PIs
predict(logM1, newdata = data.frame(x = 800), interval = "prediction",level = 0.90)
```

##Miguel: The "log" replication looks really good!

Looking at your two models, which do you think is better? Why?
#seems like the log model works better - it gave a higher r2 value aka a stronger linear relationship 


##Overall Peer Commentary: I think you did a really good job!

#It is true that in the first question the structure is a bit messy and that maybe there are some things to check, but I think the structure of second part was nice and the code was good. I missed your "problems and solutions" section too, but in general you did pretty well!