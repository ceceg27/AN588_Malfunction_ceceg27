#homework 4
[1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:
```{r}
z.prop.test <- function(p1, n1, p0, p2= NULL, n2= NULL, alternative="two.tailed", conf.level=0.95){
  #error messages for rule of thumb
   if (is.null(p2)|is.null(p1)) { 
    #Check for validity of CLT using rule of thumb 
 np <- n1 * p1 
 thumb <- n1 * (1-p1)
 
  #function for z test if there are no errors
    z<- (p1-p0)/sqrt((p0*(1-p0))/n1) #calculate z score
    
       if (alternative == 'less') {
     p <- pnorm(Z, lower.tail = TRUE) #p value and the 3 tests 
   } 
   if (alternative == 'greater') {
     p <- pnorm(Z, lower.tail = FALSE)
   }
   if (alternative == 'two.sided') {
     p <- 2 * (1-pnorm(Z))
   }
    #Ci calculation
    lwr <- (p1-qnorm(conf.level) * sqrt(p1*(1-p1))/n1) #confidence intervals
    upr <- (p1+qnorm(conf.level) * sqrt(p1*(1-p1))/n1)
    ci <- c(lwr, upr)
   
  values<- c("z-stat", z, "p value", p, "conf. intervals", ci)
print(values)
    if (np <= 5 | thumb <= 5) {  message('Warning: CLT violated.')
    }
}
#this is for two sample

  else {
    #Check Rule of Thumb 
  np1 <- n1 * p1 
 thumb1 <- n1 * (1-p1)
 np2 <- n2 * p2 
 thumb2 <- n2 * (1-p2)
 
     pstar<- (sum(p1*n1)+sum(p2*n2))/(n1+n2) #for a pooled proportion

     z<- (p1 - p2)/sqrt((pstar * (1-pstar)) * (1/length(n1) + 1/length(n2))) #z score  
     
    if (alternative == 'less') { #hypothesis being tested
     p <- pnorm(Z, lower.tail = TRUE) #p value 
   } 
   if (alternative == 'greater') {
     p <- pnorm(Z, lower.tail = FALSE)
   }
   if (alternative == 'two.sided') {
     p <- 2 * (1-pnorm(Z))
   }

#CI calculation
    lwr <- ((p2-p1)-qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1) # ci intervals
    upr <- ((p2-p1)+qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1)
    ci <- c(lwr, upr)

values<- c("z-stat", z, "p value", p, "conf. intervals", ci)
print(values)

    if (np <= 5 | thumb <= 5) {  message('Warning: CLT violated.')
}
}
}
```

*Should the second check for CLT be using np2 and thumb2? other than that, this looks good! But, you need to test the function to see if it actually works!*

[2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):

```{r}
#bring in packages
library(curl)
library(ggplot2)

#load data
f <- curl('https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv')
d <- read.csv(f, stringsAsFactors = FALSE, header = TRUE)
head(d)

#prep the data
#remove na
c<-na.omit(d) 
#make it a data frame so ggplot likes it
h<-data.frame(c) 
#name our variables so we don't have to keep retyping them 
x<-d$MaxLongevity_m 
y<-d$Brain_Size_Species_Mean 
```

```{r}
#making the regular model, using lm
M1<-lm(data = d, y~x) 
M1
summary(M1)
#r2 is .4887

#equation to put onto graph
lm_eqn <- function(m) {
a = format(coef(m)[1], digits = 2)
b = format(abs(coef(m)[2]), digits = 2)
  if (coef(m)[2] >= 0)  {
    eq <- paste('y','=', a, '+', b, 'x')
  } else {
    eq <- paste('y','=', a, '-', b, 'x') 
eq
  }
}

#making the ggplot of the first model 
ggM1<-ggplot(data = d, aes(x = x, y = y)) + geom_point() + geom_smooth(method = "lm", formula = y ~ x)+ geom_text(x = 200, y = 750, label= lm_eqn(m))
ggM1
```

*it looks like you need to define "m" in your function? Assuming m is supposed to be the model, the function should be taking the data (i.e. function("data object")) rather than the model itself. Right now, it is telling me object m not found*

Identify and interpret the point estimate of the slope (β1
), as well as the outcome of the test associated with the hypotheses H0: β1
 = 0; HA: β1
 ≠ 0. Also, find a 90 percent CI for the slope (β1
) parameter.
```{r}
#finding beta0 and beta1 
t1 <- unlist(M1$coefficients)
# unlist to get coefs out of the model
beta0<-round(t1[1],digits = 2)
beta0
beta1<-round(t1[2],digits = 2)
beta1
```
#interpretation: for every 1 g brain size increases, the animal is expected to live 1.2 months longer buttt brain size at the y intercept is 249 aka the an animal with no brain will still live 249 months, doesn't make much sense

*This may just be the skew in the data (i.e. we dont have any indiviudals in the data with no brain). Also from what I'm seeing these arent the beta values I am seeing. you may have your x and y mixed up*

```{r}
#find the 90% confidence intervals
ci.slope<-confint(M1, level = 0.9)
ci.slope
```
Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.
```{r}
#confidence intervals for 90%
ci <- predict(M1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  
ci.frame<-as.data.frame(ci,col.names = c('fit', 'lwr', 'upr'))

#prediction intervals for 90%
pi <- predict(M1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  
pi.frame<-as.data.frame(pi, col.names = c('fit', 'lwr', 'upr'))

#also need to combine the original x and y values
r<-cbind(x, y) 
#now we can combine the x and y values, and the ci and pi data frames
New<-cbind(r, ci.frame, pi.frame)
#renaming the columns
names(New) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(New)
#making the ggplot
gMod2<-ggplot(data = New, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = New, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = New, aes(x = x, y = CIlower), colour = "blue") +
  geom_line(data = New, aes(x = x, y = CIupper), colour = "blue") +
  geom_line(data = New, aes(x = x, y = PIlower), colour = "red") +
  geom_line(data = New, aes(x = x, y = PIupper), colour = "red")
gMod2
#upper and lower ci and pi are on the graph!
```

*great!*

Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
```{r}
#point estimate
predict(M1, newdata = data.frame(x = 800))
#result is 258.2979
#PIs
predict(M1, newdata = data.frame(x = 800), interval = "prediction",level = 0.90)
#fit = 258.2979, lwr = 166.6757, upr = 349.9201
```
#Interpretation: I would not expect this to be accurate, as the model is meant to predict brain sizes and longevity of animals with similar brain sizes. Because 800 grams is so much more than the brain sizes in the data set, I don't think its accurate to assume the same relationship would exist with an extremely large value. 

#now lets do it all for the log model 
```{r}
#log transform the variables and make them into a data frame 
r1<-log(d$MaxLongevity_m)
r2<-log(d$Brain_Size_Species_Mean)
df2<-as.data.frame(cbind(r2,r1))

#making the transformed model
logM1<-lm(data = d, log(y)~log(x))
logM1
summary(logM1)
#the r2 is now 0.5751, higher than it was for the regular model

#making the ggplot
ggM2 <- ggplot(data=info, aes(x=r2,y=r1))+xlab("log(Brain_Size_Species_Mean)")+ylab("log(MaxLongevity_m)")+ geom_point() + geom_smooth(method="lm", fullrange=TRUE)+ geom_text(x = 1.5, y = 6, label= lm_eqn(logM1))
ggM2

```

*what is "info"?*

```{r}
#finding beta0 and beta1 
t1 <- unlist(logM1$coefficients)
# unlist to get coefs out of the model
beta0<-round(t1[1],digits = 2)
beta0
beta1<-round(t1[2],digits = 2)
beta1
```
#interpretation: log transformation means we can look at things as %. for every 1% increase in brain size there is a .2% increase in lifespan 

#find the 90% confidence intervals
ci.slope<-confint(M1, level = 0.9)
ci.slope

```{r}
#confidence intervals for 90%
ci <- predict(logM1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  
logci.frame<-data.frame(ci)

#prediction intervals for 90%
pi <- predict(logM1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  
logpi.frame<-data.frame(pi)

#also need to combine the original x and y values
r<-cbind(x, y) 
#now we can combine the x and y values, and the ci and pi data frames
logNew<-cbind(r, ci.frame, pi.frame)
#renaming the columns
names(logNew) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(logNew)
#making the ggplot
loggMod2<-ggplot(data = logNew, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = logNew, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = logNew, aes(x = x, y = CIlower), colour = "blue") +
  geom_line(data = logNew, aes(x = x, y = CIupper), colour = "blue") +
  geom_line(data = logNew, aes(x = x, y = PIlower), colour = "red") +
  geom_line(data = logNew, aes(x = x, y = PIupper), colour = "red")
loggMod2
```

```{r}
#point estimate
predict(logM1, newdata = data.frame(x = 800))
#result is 258.2979
#PIs
predict(logM1, newdata = data.frame(x = 800), interval = "prediction",level = 0.90)
```

Looking at your two models, which do you think is better? Why?
#seems like the log model works better - it gave a higher r2 value aka a stronger linear relationship 

#challenges I faced
- Part 1 was really difficult for me, I think I got really disorganized and did everything a much harder way than it needed to be done. Looking at my group mates codes helped out a lot. I found a way to condense and organize my code so it wasn't so hard to understand
- I am still not totally sure if part 1 is done right, I definitely need more work with function building
- I struggled at first to interprey b1 and b0, but after some googling I think i figured it out
- I don't think I was able to properly add a legend for my graph in part 2 with the ci and pi lines, not sure how to do this still *pay close attention to what data you are calling on n your ggplot functions*
- Overall I think I just need to work on organizing and condensing my work when writing functions


