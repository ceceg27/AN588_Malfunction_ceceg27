---
title: "ceceg27_PeerCommentary_vfrench3_04"
author: "Victoria French"
date: "10/23/2021"
output: html_document
---

# Homework 4

[1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:
```{r}
z.prop.test <- function(p1, n1, p0, p2= NULL, n2= NULL, alternative="two.tailed", conf.level=0.95){
  #error messages for rule of thumb
   if (n1*p0 < 5) #V: So for the rule of thumb you are multiplying the n value times the associated sample proportion, not the null value, but your equation is correct! 
  {
  print("Non Normal!")
  }
if (n1*(1-p0) < 5) #V: To simplify your code you could include this in the original if statement conditions by using an OR justification (|)
  {
  print("Non Normal!")
  }
#these only need to show up if using p2 and n2
if (!is.null(n2))
{
  if (n2*p0 < 5) #V: Again compare to p2 not p0
  {
  print("Non Normal!")
  }
  if (n2*(1-p0) < 5)
  {
  print("Non Normal!")
  }
  #function for z test if there are no errors
  if (alternative == "two.tailed")
  {
    z<- (p1-p0)/sqrt((p0*(1-p0))/n1) #z score
    p<- pnorm(z, lower.tail = TRUE) #p value
    #V: The p value is what changes depending on what test you run. Here you are calculating a p value for a lower tailed test, not a two tailed. 
    lwr <- (p1-qnorm(conf.level) * sqrt(p1*(1-p1))/n1) #confidence intervals
    #V: When calculating the confidence intervals the qnorm function is calculating a quantile based on the alpha value, not the confidence level. The alpha value is just 1-conf.level. 
    upr <- (p1+qnorm(conf.level) * sqrt(p1*(1-p1))/n1)
    ci <- c(lwr, upr)
   }
#this is for one sample, when n2 and p2 are null
  if (is.null(n2)|is.null(p2))
  {
    z<- (p1-p0)/sqrt((p0*(1-p0))/n1) 
    p<- pnorm(z, lower.tail = TRUE)
    lwr <- (p1-qnorm(conf.level) * sqrt(p1*(1-p1))/n1)
    upr <- (p1+qnorm(conf.level) * sqrt(p1*(1-p1))/n1)
    ci <- c(lwr, upr)
#V: So you are missing the upper tail test for the single proportions. (The chunk above this one is the lower tail test for single proportions)
  }
#this is for two sample, if p1 is less than p2
    if (alternative == "less")
  {
    pstar<- (sum(p1*n1)+sum(p2*n2))/(n1+n2) #for a pooled proportion
    z<- (p2 - p1)/sqrt((pstar * (1-pstar)) * (1/length(n1) + 1/length(n2))) #z score
    p<- 1-pnorm(z, lower.tail = T) + pnorm(z,lower.tail = F) #p value
    #V: This is calculating for a two tailed test not a lower.tail
    lwr <- ((p2-p1)-qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1) # ci intervals
    upr <- ((p2-p1)+qnorm(conf.level) * sqrt((p2-p1)*(1-(p2-p1)))/n1)
    ci <- c(lwr, upr)
  }
 #this is for two sample, if p1 is greater than p2
   if (alternative == "greater")
  {
    pstar<- (sum(p1*n1)+sum(p2*n2))/(n1+n2) #for a pooled proportion
    #V: I don't know if the sum functions are necessary? 
    z<- (p1 - p2)/sqrt((pstar * (1-pstar)) * (1/length(n1) + 1/length(n2))) #z score
    #V: n1 and n2 should already be the lengths of your dataset when you argue them in the function. If you take the length of n1 and n2 it would just be 1? 
    p<- 1-pnorm(z, lower.tail = T) + pnorm(z,lower.tail = F) #p value
    lwr <- ((p1-p2)-qnorm(conf.level) * sqrt((p1-p2)*(1-(p1-p2)))/n1) #ci intervals
    upr <- ((p1-p2)+qnorm(conf.level) * sqrt((p1-p2)*(1-(p1-p2)))/n1)
    ci <- c(lwr, upr)
    
#V: So you are missing a two tailed test for the two sample proportions
  }
values<- c("z-stat", z, "p value", p, "conf. intervals", ci)
print(values)
}
}

```

V: This code is a little confusing for me becuase you jump around between tests alot. I think you are making the assumption that alot of people make, that the alternative hypotheses are related to the amount of samples you are testing. From my understanding, you can conduct an upper, lower and two tailed tests in both scenarios. The test just indicates what alternative hypothesis you are testing. Where as the samples determines the null hypothesis. For one sample you are
seeing if the sample is (greater, less than, or different) to the null (p0). For two sample you are seeing if the difference between the sample proportions are (greater, less than, or different) than zero. For the different sampling tests you will calculate different Z statistics. For each alternative hypothesis within the sampling test you will calculate a different p-value based on that z statistic. Hopefully this explanation helps and makes sense. 


[2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):

```{r}
#bring in packages
library(curl)
library(ggplot2)

#load data
f <- curl('https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv')
d <- read.csv(f, stringsAsFactors = FALSE, header = TRUE)
head(d)

#prep the data
#remove na
c<-na.omit(d) #V: The lm function will remove NAs automatically but I like that you had the forethought to do it yourself! 

#make it a data frame so ggplot likes it
h<-data.frame(c) 
#name our variables so we dont have to keep retyping them 
x<-d$MaxLongevity_m 
y<-d$Brain_Size_Species_Mean 
#V: So since we are predicting longevity (dependent variable) FROM brainsize (independent variable) I think these variables should be flipped? meaning the y should be longevity while x is brainsize. 
```

```{r}
#making the regular model, using lm
M1<-lm(data = d, y~x) 
M1
summary(M1)
#r2 is .4887 #V: I like that you highlight what parts of the summary you think are important. 

#making the ggplot of the first model 
ggM1<-ggplot(data = d, aes(x = x, y = y)) + geom_point() + geom_smooth(method = "lm", formula = y ~ x)
ggM1
#V: Missing the fitted model equation appended to the graph!
```

Identify and interpret the point estimate of the slope (β1
), as well as the outcome of the test associated with the hypotheses H0: β1
 = 0; HA: β1
 ≠ 0. Also, find a 90 percent CI for the slope (β1
) parameter.
```{r}
#finding beta0 and beta1 
t1 <- unlist(M1$coefficients)
# unlist to get coefs out of the model
beta0<-round(t1[1],digits = 2)
beta0
beta1<-round(t1[2],digits = 2)
beta1
#find the 90% confidence intervals
ci.slope<-confint(M1, level = 0.9)
ci.slope
```
V: Missing your interpretation of the coefficients and your conclusion from the hypothesis test.  

Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.
```{r}
#confidence intervals for 90%
ci <- predict(M1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  
ci.frame<-data.frame(ci)

#prediction intervals for 90%
pi <- predict(M1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  
pi.frame<-data.frame(pi)

#also need to combine the original x and y values
r<-cbind(x, y) 
#now we can combine the x and y values, and the ci and pi data frames
New<-cbind(r, ci.frame, pi.frame)
#renaming the columns
names(New) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(New)

#making the ggplot
gMod2<-ggplot(data = New, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = New, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = New, aes(x = x, y = CIlower), colour = "blue") +
  geom_line(data = New, aes(x = x, y = CIupper), colour = "blue") +
  geom_line(data = New, aes(x = x, y = PIlower), colour = "red") +
  geom_line(data = New, aes(x = x, y = PIupper), colour = "red")
gMod2
#upper and lower ci and pi are on the graph!

#V: You are missing the legend to distinguish between PI and CI
```

Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
```{r}
#point estimate
predict(M1, newdata = data.frame(x = 800))
#result is 258.2979
#PIs
predict(M1, newdata = data.frame(x = 800), interval = "prediction",level = 0.90)
#fit = 258.2979, lwr = 166.6757, upr = 349.9201
#I would not expect this to be accurate, as the model is meant to predict brain sizes and longevity of animals with similar brain sizes. Because 800 grams is so much more than the brain sizes in the data set, I don't think its accurate to assume the same relationship would exist with an extremely large value. 
```

#now lets do it all for the log model 
```{r}
#log transform the variables and make them into a data frame 
r1<-log(d$MaxLongevity_m)
r2<-log(d$Brain_Size_Species_Mean)
df2<-as.data.frame(cbind(r2,r1))

#making the transformed model
logM1<-lm(data = d, log(y)~log(x))
logM1
summary(logM1)
#the r2 is now 0.5751, higher than it was for the regular model

#making the ggplot
ggM2 <- ggplot(data=info, aes(x=r2,y=r1))+xlab("log(Brain_Size_Species_Mean)")+ylab("log(MaxLongevity_m)")+ geom_point() + geom_smooth(method="lm", fullrange=TRUE)
ggM2

#V: I'm getting an error running this, says 'Object: "info" not found?'. Maybe it is just my computer? 

```

```{r}
#finding beta0 and beta1 
t1 <- unlist(logM1$coefficients)
# unlist to get coefs out of the model
beta0<-round(t1[1],digits = 2)
beta0
beta1<-round(t1[2],digits = 2)
beta1
#find the 90% confidence intervals
ci.slope<-confint(M1, level = 0.9)
ci.slope
```

```{r}
#confidence intervals for 90%
ci <- predict(logM1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  
logci.frame<-data.frame(ci)

#prediction intervals for 90%
pi <- predict(logM1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  
logpi.frame<-data.frame(pi)

#also need to combine the original x and y values
r<-cbind(x, y) 
#now we can combine the x and y values, and the ci and pi data frames
logNew<-cbind(r, ci.frame, pi.frame)
#renaming the columns
names(logNew) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(logNew)
#making the ggplot
loggMod2<-ggplot(data = logNew, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = logNew, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = logNew, aes(x = x, y = CIlower), colour = "blue") +
  geom_line(data = logNew, aes(x = x, y = CIupper), colour = "blue") +
  geom_line(data = logNew, aes(x = x, y = PIlower), colour = "red") +
  geom_line(data = logNew, aes(x = x, y = PIupper), colour = "red")
loggMod2
```

```{r}
#point estimate
predict(logM1, newdata = data.frame(x = 800))
#result is 258.2979
#PIs
predict(logM1, newdata = data.frame(x = 800), interval = "prediction",level = 0.90)
```

Looking at your two models, which do you think is better? Why?
#seems like the log model works better - it gave a higher r2 value aka a stronger linear relationship 

# Overall Peer Comments: 

- The code for the Z.prop.test function was a little confusing for me to read. While the code for the linear regression section was very concise and clear!
- I would just remember to add in a few of the elements the questions are asking for. I highlighted them where I saw them throughout the script. Mostly just the legend on the ggplot objects and the interpretations! 
- The final assignment should also include a challenges section if you had any particular area where you were struggling!
- Overall Great Job! 
