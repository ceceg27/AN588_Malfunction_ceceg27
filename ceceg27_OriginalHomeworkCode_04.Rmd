#homework 4
[1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:
```{r}



[2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):

```{r}
#bring in packages
library(curl)
library(ggplot2)

#load data
f <- curl('https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv')
d <- read.csv(f, stringsAsFactors = FALSE, header = TRUE)
head(d)

#prep the data
#remove na
c<-na.omit(d) 
#make it a data frame so ggplot likes it
h<-data.frame(c) 
#name our variables so we dont have to keep retyping them 
x<-d$MaxLongevity_m 
y<-d$Brain_Size_Species_Mean 

```

```{r}
#making the regular model, using lm
M1<-lm(data = d, y~x) 
M1
summary(M1)
#r2 is .4887

#making the ggplot of the first model 
ggM1<-ggplot(data = d, aes(x = x, y = y)) + geom_point() + geom_smooth(method = "lm", formula = y ~ x)
ggM1
```

Identify and interpret the point estimate of the slope (β1
), as well as the outcome of the test associated with the hypotheses H0: β1
 = 0; HA: β1
 ≠ 0. Also, find a 90 percent CI for the slope (β1
) parameter.
```{r}
#finding beta0 and beta1 
t1 <- unlist(M1$coefficients)
# unlist to get coefs out of the model
beta0<-round(t1[1],digits = 2)
beta0
beta1<-round(t1[2],digits = 2)
beta1
#find the 90% confidence intervals
ci.slope<-confint(M1, level = 0.9)
ci.slope
```
Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.
```{r}
#confidence intervals for 90%
ci <- predict(M1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  
ci.frame<-data.frame(ci)

#prediction intervals for 90%
pi <- predict(M1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  
pi.frame<-data.frame(pi)

#also need to combine the original x and y values
r<-cbind(x, y) 
#now we can combine the x and y values, and the ci and pi data frames
New<-cbind(r, ci.frame, pi.frame)
#renaming the columns
names(New) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(New)
#making the ggplot
gMod2<-ggplot(data = New, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = New, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = New, aes(x = x, y = CIlower), colour = "blue") +
  geom_line(data = New, aes(x = x, y = CIupper), colour = "blue") +
  geom_line(data = New, aes(x = x, y = PIlower), colour = "red") +
  geom_line(data = New, aes(x = x, y = PIupper), colour = "red")
gMod2
#upper and lower ci and pi are on the graph!
```
Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
```{r}
#point estimate
predict(M1, newdata = data.frame(x = 800))
#result is 258.2979
#PIs
predict(M1, newdata = data.frame(x = 800), interval = "prediction",level = 0.90)
#fit = 258.2979, lwr = 166.6757, upr = 349.9201
#I would not expect this to be accurate, as the model is meant to predict brain sizes and longevity of animals with similar brain sizes. Because 800 grams is so much more than the brain sizes in the data set, I don't think its accurate to assume the same relationship would exist with an extremely large value. 
```
#now lets do it all for the log model 
```{r}
#log transform the variables and make them into a data frame 
r1<-log(d$MaxLongevity_m)
r2<-log(d$Brain_Size_Species_Mean)
df2<-as.data.frame(cbind(r2,r1))

#making the transformed model
logM1<-lm(data = d, log(y)~log(x))
logM1
summary(logM1)
#the r2 is now 0.5751, higher than it was for the regular model

#making the ggplot
ggM2 <- ggplot(data=info, aes(x=r2,y=r1))+xlab("log(Brain_Size_Species_Mean)")+ylab("log(MaxLongevity_m)")+ geom_point() + geom_smooth(method="lm", fullrange=TRUE)
ggM2

```

```{r}
#finding beta0 and beta1 
t1 <- unlist(logM1$coefficients)
# unlist to get coefs out of the model
beta0<-round(t1[1],digits = 2)
beta0
beta1<-round(t1[2],digits = 2)
beta1
#find the 90% confidence intervals
ci.slope<-confint(M1, level = 0.9)
ci.slope
```

```{r}
#confidence intervals for 90%
ci <- predict(logM1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  
logci.frame<-data.frame(ci)

#prediction intervals for 90%
pi <- predict(logM1, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  
logpi.frame<-data.frame(pi)

#also need to combine the original x and y values
r<-cbind(x, y) 
#now we can combine the x and y values, and the ci and pi data frames
logNew<-cbind(r, ci.frame, pi.frame)
#renaming the columns
names(logNew) <- c("x", "y", "CIfit", "CIlower", "CIupper", "PIfit", "PIlower", "PIupper")
head(logNew)
#making the ggplot
loggMod2<-ggplot(data = logNew, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = logNew, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = logNew, aes(x = x, y = CIlower), colour = "blue") +
  geom_line(data = logNew, aes(x = x, y = CIupper), colour = "blue") +
  geom_line(data = logNew, aes(x = x, y = PIlower), colour = "red") +
  geom_line(data = logNew, aes(x = x, y = PIupper), colour = "red")
loggMod2
```

```{r}
#point estimate
predict(logM1, newdata = data.frame(x = 800))
#result is 258.2979
#PIs
predict(logM1, newdata = data.frame(x = 800), interval = "prediction",level = 0.90)
```
Looking at your two models, which do you think is better? Why?
#seems like the log model works better - it gave a higher r2 value aka a stronger linear relationship 


